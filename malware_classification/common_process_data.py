import malware_classification.global_var as GLVAR
import numpy as np
import json
from sklearn.model_selection import train_test_split
import math
import re


def generate_ngrams(s, n):
    # Convert to lowercases
    s = s.lower()

    # Replace all none alphanumeric characters with spaces
    s = re.sub(r'[^a-zA-Z0-9\s]', ' ', s)

    # Break sentence in the token, remove empty tokens
    tokens = [token for token in s.split(" ") if token != ""]

    # Use the zip function to help us generate n-grams
    # Concatentate the tokens into ngrams and return
    # return list demo:"aa-bb-cc bb-cc-dd ..."
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return ["-".join(ngram) for ngram in ngrams]


def create_n_gramed_data(raw_api_filename, no_repet_n_gramed_data_filename, n):
    apis = []
    with open(raw_api_filename) as raw_data:
        for line in raw_data:
            line_n_gram = generate_ngrams(line, n)
            last_gram = ""
            tmp_no_repet_n_gram = []
            for gram in line_n_gram:
                if gram == last_gram:
                    continue
                else:
                    tmp_no_repet_n_gram.append(gram)
                    last_gram = gram
            line_no_repet_n_gram = " ".join(tmp_no_repet_n_gram)
            apis.append(line_no_repet_n_gram)
    apis_str = "\n".join(apis)
    with open(no_repet_n_gramed_data_filename, 'w')as g:
        g.write(apis_str)
        g.close()


def remove_repet(_list, width):
    n = len(_list)
    if n <= 1:
        print(_list)
        return
    list_rm_repet = []
    for i in range(n - 2 * width + 1):
        isRepet = False
        for j in range(width):
            if _list[i + j] == _list[i + j + width]:
                isRepet = True
            else:
                break
        if isRepet == False:
            list_rm_repet.append(_list[i])
    list_rm_repet.extend(_list[-width:])
    return list_rm_repet


def remove_repet_of_raw_data(raw_api_filename, no_repet_data_filename, long_width):
    print("running remove_repet_of_raw_data ...")
    with open(raw_api_filename) as raw_data:
        apis = []
        count = 0
        for line in raw_data:
            line_list = line.replace('\n', '').split(' ')
            n = len(line_list)
            # if n <= 0:
            #     print(line_list)
            #     continue
            count += 1
            if count % 100 ==0:
                print(count)
            for i in range(long_width):
                if n <= GLVAR.pic_pow_size * GLVAR.pic_pow_size:
                    break
                line_list = remove_repet(line_list, i + 1)
                n = len(line_list)

            if n > GLVAR.pic_pow_size*GLVAR.pic_pow_size:
                for i in range(long_width):
                    if n <= GLVAR.pic_pow_size * GLVAR.pic_pow_size:
                        break
                    line_list = remove_repet(line_list, i + 1)
                    n = len(line_list)
            apis.append(" ".join(line_list))
    apis_str = "\n".join(apis)
    with open(no_repet_data_filename, 'w')as g:
        g.write(apis_str)
        g.close()


def create_oprations_set(raw_filename, op_index_filename):
    print("running create_oprations_set ...")
    with open(raw_filename) as raw_data:
        operations = set()
        for line in raw_data:
            operation = line.replace('\n', '').split(' ')
            operations = operations.union(set(operation))

    operations_list = list(operations)
    map_index = range(len(operations))
    ziped_op_index = zip(operations_list, map_index)

    operations_dic = {k: v + 1 for k, v in ziped_op_index}  # V+1 for no operation index is 0

    with open(op_index_filename, 'w') as json_file:
        json.dump(operations_dic, json_file, ensure_ascii=False)
    print("operations index dictionary create success! Dic file saved in ", op_index_filename)
    print("the operations's count is:", len(operations))


def raw_labels_to_index(raw_lable_filename):
    raw_lables_list = []

    with open(raw_lable_filename) as raw_data:
        for line in raw_data:
            raw_lables_list.append(line.replace('\n', '').strip())

    lables_set_index = {}
    for lable in raw_lables_list:
        if lable not in lables_set_index:
            lables_set_index[lable] = len(lables_set_index)
            print(lable, "--", lables_set_index[lable])
    print("the lables's count is:", len(lables_set_index))
    lables_index_np = np.zeros(len(raw_lables_list))
    for i, lable in enumerate(raw_lables_list):
        lables_index_np[i] = lables_set_index.get(lable)
    return lables_index_np


def load_npz_data(file_path):
    f = np.load(file_path)
    x_train, y_train = f['x_train'], f['y_train']
    x_test, y_test = f['x_test'], f['y_test']
    f.close()
    return (x_train, y_train), (x_test, y_test)


def process_raw_data_4_attention(raw_api_filename, raw_lable_filename, op_index_filename,
                                 attention_train_data):
    print("running process_raw_data_4_attention ...")
    np.set_printoptions(threshold=np.inf)

    with open(op_index_filename, 'r') as fileR:
        operation_dic = json.load(fileR)
        fileR.close()
        print("the total operations count is:", len(operation_dic))

    with open(raw_api_filename) as raw_data:
        line_num = len(raw_data.readlines())
        print("raw data total data count is:", line_num)

    with open(raw_api_filename) as raw_data:
        longest_operation_size = 0
        for i, line in enumerate(raw_data):
            operation = str(line).split(' ')
            tmp_len = len(operation)
            if tmp_len > longest_operation_size:
                longest_operation_size = tmp_len
                if tmp_len == 12179:
                    print(line)
                    print(i)
        print("longest data RAW operation length is:", longest_operation_size)
        longest_operation_size = pow(math.ceil(math.sqrt(longest_operation_size)), 2)
        print("longest data operation length is:", longest_operation_size)
        print("picture size is: ", math.sqrt(longest_operation_size), " * ",
              math.sqrt(longest_operation_size))
    with open(raw_api_filename) as raw_data:
        print("the total operations's ocunt is:", len(operation_dic),
              "\n the longest operation length is",
              longest_operation_size)
        processed_data_np = np.empty(shape=(line_num, longest_operation_size)).astype("int32")
        count = 0
        for i, line in enumerate(raw_data):
            count += 1
            if count % 100 == 0:
                print(count)
            tmp_processed_data = [0 for x in range(0, longest_operation_size)]
            operation = line.replace('\n', '').split(' ')
            j = 0
            for op in operation:
                if len(op) != 0:
                    index = operation_dic[op]
                    tmp_processed_data[j] = index
                    j += 1
            processed_data_np[i] = np.array(tmp_processed_data)
        labels_index_np = raw_labels_to_index(raw_lable_filename)

        x_train, x_test, y_train, y_test = train_test_split(processed_data_np, labels_index_np,
                                                            test_size=0.2,
                                                            random_state=0)

        np.savez(attention_train_data, x_train=x_train, x_test=x_test, y_train=y_train,
                 y_test=y_test)

def main():
    long_width = 100


    remove_repet_of_raw_data(GLVAR.RAW_API_FILENAME, GLVAR.NO_REPET_DATA_FINAME, long_width)

    create_oprations_set(GLVAR.NO_REPET_DATA_FINAME, GLVAR.OP_INDEX_FILENAME)

    process_raw_data_4_attention(GLVAR.NO_REPET_DATA_FINAME, GLVAR.RAW_LABLE_FILENAME, GLVAR.OP_INDEX_FILENAME,
                                 GLVAR.TRAIN_AND_TEST_DATA)
    # (X_Train, y_Train), (X_Test, y_Test) = load_npz_data("F:/数据集/Kim2016/malware_dataset/malware_dataset/train_test_data_final.npz")


if __name__ == "__main__":
    main()
