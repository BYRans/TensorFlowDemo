# %%
from keras.models import model_from_json
from keras.utils import np_utils
from matplotlib import pyplot as plt
import time

from keras import backend as K
from keras.engine.topology import Layer
from malware_classification import common_process_data as read_data

class Self_Attention(Layer):

    def __init__(self, output_dim, **kwargs):
        self.output_dim = output_dim
        super(Self_Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        # 为该层创建一个可训练的权重
        # inputs.shape = (batch_size, time_steps, seq_len)
        self.kernel = self.add_weight(name='kernel',
                                      shape=(3, input_shape[2], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)

        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它

    def call(self, x):
        WQ = K.dot(x, self.kernel[0])
        WK = K.dot(x, self.kernel[1])
        WV = K.dot(x, self.kernel[2])

        print("WQ.shape", WQ.shape)

        print("K.permute_dimensions(WK, [0, 2, 1]).shape", K.permute_dimensions(WK, [0, 2, 1]).shape)

        QK = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1]))

        QK = QK / (64 ** 0.5)

        QK = K.softmax(QK)

        print("QK.shape", QK.shape)

        V = K.batch_dot(QK, WV)

        return V

    def get_config(self):
        config = {
            'output_dim': self.output_dim
        }
        base_config = super(Self_Attention, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], self.output_dim)

num_classes=13
max_features = 983 # 该数要比operation的个数大1
epochs=30

print('Loading data...')

(x_train, y_train), (x_test, y_test) = read_data.load_npz_data("F:/数据集/Kim2016/malware_dataset/malware_dataset/attention_train_test_data_final.npz")
X_train = x_train.reshape(-1,625)  # why / 255?
X_test = x_test.reshape(-1,625)
# 标签转换为独热码
y_train = np_utils.to_categorical(y_train, num_classes=num_classes)
y_test = np_utils.to_categorical(y_test, num_classes=num_classes)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

# %%数据归一化处理

maxlen = 625

print('x_train shape:', x_train.shape)

print('x_test shape:', x_test.shape)



# %%

batch_size = 32
from keras.models import Model
from keras.optimizers import SGD, Adam
from keras.layers import *
# from Attention_keras import Attention, Position_Embedding

S_inputs = Input(shape=(625,), dtype='int32')

embeddings = Embedding(max_features, 128)(S_inputs)

O_seq = Self_Attention(128)(embeddings)

O_seq = GlobalAveragePooling1D()(O_seq)

O_seq = Dropout(0.5)(O_seq)

SA_outputs = Dense(625, activation='relu')(O_seq)

SA_dense_output = Reshape((25, 25, 1))(SA_outputs)


# build CNN model linked RNN outputs
x = Conv2D(filters=64,
           kernel_size=(5,5),
           padding='same',
           input_shape=(25,25, 1),
           activation='relu',
           name='conv2d_1')(SA_dense_output)
x = MaxPool2D(pool_size=(2,2), name='max_pooling2d_1')(x)
x = Conv2D(filters=64,
           kernel_size=(5,5),
           padding='same',
           input_shape=(25,25, 1),
           activation='relu',
           name='conv2d_2')(x)
x = MaxPool2D(pool_size=(2,2), name='max_pooling2d_2')(x)
x = Conv2D(filters=32,
           kernel_size=(5,5),
           padding='same',
           input_shape=(25,25, 1),
           activation='relu',
           name='conv2d_3')(x)
x = MaxPool2D(pool_size=(2,2), name='max_pooling2d_3')(x)
x = Dropout(0.25)(x)
x = Flatten()(x)
x = Dense(625)(x)
x = Dropout(0.5)(x)
# output = TimeDistributed(Dense(num_classes, activation='softmax'))(x)
output = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=S_inputs, outputs=output)


print(model.summary())
# try using different optimizers and different optimizer configs
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# %%
print('Train...')

h = model.fit(x_train, y_train,

              batch_size=batch_size,

              epochs=epochs,

              validation_data=(x_test, y_test))

plt.plot(h.history["loss"], label="train_loss")
plt.plot(h.history["val_loss"], label="val_loss")
plt.plot(h.history["acc"], label="train_acc")
plt.plot(h.history["val_acc"], label="val_acc")
plt.legend()
plt.show()

print('Testing--------------')
loss, accuracy = model.evaluate(X_test, y_test)

print('test loss:', loss)
print('test accuracy:', accuracy)


print("-----------------------DY Add------------------------")
import matplotlib.pyplot as plt


def show_train_history(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    title = 'Train History of Self-Attention: epochs-' + str(epochs) + " " + str(time.strftime("%Y-%m-%d %X", time.localtime()))
    plt.title(title)
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()


show_train_history(h, 'acc', 'val_acc')

show_train_history(h, 'loss', 'val_loss')

scores = model.evaluate(X_test, y_test)
print()
print("\t[Info] Accuracy of testing data = {:2.1f}%".format(scores[1] * 100.0))




# serialize model to JSON
model_json = model.to_json()
with open("F:/数据集/Kim2016/malware_dataset/malware_dataset/self_attention_model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("F:/数据集/Kim2016/malware_dataset/malware_dataset/self_attention_model_weights.h5")
print("Saved model to disk")


