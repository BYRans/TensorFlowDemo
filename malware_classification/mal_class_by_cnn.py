import numpy as np
import json
from sklearn.model_selection import train_test_split
import math
from keras.utils import np_utils

def create_oprations_set(raw_filename, op_index_filename):
    with open(raw_filename) as raw_data:
        operations = set()
        for line in raw_data:
            operation = line.replace('\n', '').replace('"', '').split(',')
            del operation[0]
            del operation[0]
            operations = operations.union(set(operation))

    operations_list = list(operations)
    map_index = range(len(operations))
    ziped_op_index = zip(operations_list, map_index)

    operations_dic = {k: v for k, v in ziped_op_index}

    with open(op_index_filename, 'w') as json_file:
        json.dump(operations_dic, json_file, ensure_ascii=False)
    print("operations index dictionary create success! Dic file saved in ", op_index_filename)
    print("the operations's count is:",len(operations))


def raw_labels_to_index(raw_lables_list):
    for i, lable in enumerate(raw_lables_list):
        raw_lables_list[i] = lable.split(".")[0]

    lables_set_index = {}
    for lable in raw_lables_list:
        if lable not in lables_set_index:
            lables_set_index[lable] = len(lables_set_index)
    print("the lables's count is:",len(lables_set_index))
    lables_index_np = np.zeros(len(raw_lables_list))
    for i, lable in enumerate(raw_lables_list):
        lables_index_np[i] = lables_set_index.get(lable)
    return lables_index_np


def process_raw_data(raw_filename, op_index_filename):
    np.set_printoptions(threshold=np.inf)

    with open(op_index_filename, 'r') as fileR:
        operation_dic = json.load(fileR)
        fileR.close()

    with open(raw_filename) as raw_data:
        line_num = len(raw_data.readlines())

    with open(raw_filename) as raw_data:
        length = pow(math.ceil(math.sqrt(len(operation_dic))), 2)
        print("the total operations's ocunt is:",len(operation_dic),"the picture size is",math.ceil(math.sqrt(len(operation_dic))),"*",math.ceil(math.sqrt(len(operation_dic))))
        processed_data_np = np.empty(shape=(line_num, length)).astype("int32")
        labels_list = []
        for i, line in enumerate(raw_data):
            tmp_processed_data = [0 for x in range(0, length)]
            operation = line.replace('\n', '').replace('"', '').split(',')
            labels_list.append(operation.pop(0))
            del operation[0]
            operation_set = set(operation)
            for op in operation_set:
                if len(op) != 0:
                    index = operation_dic[op]
                    tmp_processed_data[index] = 1
            processed_data_np[i] = np.array(tmp_processed_data)

        labels_index_np = raw_labels_to_index(labels_list)

        x_train, x_test, y_train, y_test = train_test_split(processed_data_np, labels_index_np, test_size=0.01,
                                                            random_state=0)

        np.savez('F:/数据集/Kim2016/malware_dataset/malware_dataset/train_test_data.npz', x_train=x_train, x_test=x_test,
                 y_train=y_train, y_test=y_test)


def load_npz_data(file_path):
    f = np.load(file_path)
    x_train, y_train = f['x_train'], f['y_train']
    x_test, y_test = f['x_test'], f['y_test']
    f.close()
    return (x_train, y_train), (x_test, y_test)


def cnn_train(data_file_path):
    (X_Train, y_Train), (X_Test, y_Test) = load_npz_data(data_file_path)

    # Translation of data
    X_Train4D = X_Train.reshape(X_Train.shape[0], 33, 33, 1).astype('float32')
    X_Test4D = X_Test.reshape(X_Test.shape[0], 33, 33, 1).astype('float32')

    # Standardize feature data
    X_Train4D_norm = X_Train4D / 255
    X_Test4D_norm = X_Test4D / 255

    # Label Onehot-encoding
    y_TrainOneHot = np_utils.to_categorical(y_Train)
    y_TestOneHot = np_utils.to_categorical(y_Test)

    from keras.models import Sequential
    from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D

    model = Sequential()
    # Create CN layer 1
    model.add(Conv2D(filters=256,
                     kernel_size=(5, 5),
                     padding='same',
                     input_shape=(33, 33, 1),
                     activation='relu',
                     name='conv2d_1'))
    # Create Max-Pool 1
    model.add(MaxPool2D(pool_size=(2, 2), name='max_pooling2d_1'))

    # Create CN layer 2
    model.add(Conv2D(filters=128,
                     kernel_size=(5, 5),
                     padding='same',
                     input_shape=(33, 33, 1),
                     activation='relu',
                     name='conv2d_2'))

    # Create Max-Pool 2
    model.add(MaxPool2D(pool_size=(2, 2), name='max_pooling2d_2'))

    # Create CN layer 3
    model.add(Conv2D(filters=64,
                     kernel_size=(5, 5),
                     padding='same',
                     input_shape=(33, 33, 1),
                     activation='relu',
                     name='conv2d_3'))

    # Create Max-Pool 3
    model.add(MaxPool2D(pool_size=(2, 2), name='max_pooling2d_3'))



    # Add Dropout layer
    model.add(Dropout(0.25, name='dropout_1'))

    model.add(Flatten(name='flatten_1'))

    model.add(Dense(128, activation='relu', name='dense_1'))
    model.add(Dropout(0.5, name='dropout_2'))

    model.add(Dense(15, activation='softmax', name='dense_2'))

    model.summary()
    print("")

    # 定義訓練方式
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 開始訓練
    train_history = model.fit(x=X_Train4D_norm,
                              y=y_TrainOneHot, validation_split=0.2,
                              epochs=100, batch_size=300, verbose=1)

    import matplotlib.pyplot as plt


    def show_train_history(train_history, train, validation):
        plt.plot(train_history.history[train])
        plt.plot(train_history.history[validation])
        plt.title('Train History')
        plt.ylabel(train)
        plt.xlabel('Epoch')
        plt.legend(['train', 'validation'], loc='upper left')
        plt.show()

    show_train_history(train_history, 'acc', 'val_acc')

    show_train_history(train_history, 'loss', 'val_loss')

    scores = model.evaluate(X_Test4D_norm, y_TestOneHot)
    print()
    print("\t[Info] Accuracy of testing data = {:2.1f}%".format(scores[1] * 100.0))

    print("\t[Info] Making prediction of X_Test4D_norm")
    prediction = model.predict_classes(X_Test4D_norm)  # Making prediction and save result to prediction
    print()
    print("\t[Info] Show 10 prediction result (From 240):")
    print("%s\n" % (prediction[240:250]))


def main():
    raw_filename = 'F:/数据集/Kim2016/malware_dataset/malware_dataset/malware_API_dataset - bak - selected.csv'
    op_index_filename = 'F:/数据集/Kim2016/malware_dataset/malware_dataset/operations_map.json'

    create_oprations_set(raw_filename, op_index_filename)
    process_raw_data(raw_filename, op_index_filename)
    (X_Train, y_Train), (X_Test, y_Test) = load_npz_data("F:/数据集/Kim2016/malware_dataset/malware_dataset/train_test_data.npz")
    cnn_train("F:/数据集/Kim2016/malware_dataset/malware_dataset/train_test_data.npz")


if __name__ == "__main__":
    main()
