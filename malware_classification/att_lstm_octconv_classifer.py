# mnist attention
import time
import numpy as np
np.random.seed(1337)
from keras.datasets import mnist
from keras.utils import np_utils
from keras.layers import *
from keras.models import *
from keras.optimizers import Adam
from malware_classification import common_process_data as read_data
from keras.layers import Input, LSTM, Bidirectional, Conv2D, Reshape
from keras import layers
from malware_classification.oct_conv2d import OctConv2D

batch_size = 64
TIME_STEPS = 25
INPUT_DIM = 25
lstm_units = 128
num_classes = 13
epochs = 100
alpha = 0.25       # alpha of OctConv
N=4                # N of OctConv
k=10               # N of OctConv

# data pre-processing
# (X_train, y_train), (X_test, y_test) = mnist.load_data('mnist.npz')
(X_train, y_train), (X_test, y_test) = read_data.load_npz_data("F:/数据集/Kim2016/malware_dataset/malware_dataset/attention_train_test_data_final.npz")
X_train = X_train.reshape(-1, 25, 25) / 255.
X_test = X_test.reshape(-1, 25, 25) / 255.
y_train = np_utils.to_categorical(y_train, num_classes=num_classes)
y_test = np_utils.to_categorical(y_test, num_classes=num_classes)
print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)

# first way attention
def attention_3d_block(inputs):
    #input_dim = int(inputs.shape[2])
    a = Permute((2, 1))(inputs)
    a = Dense(TIME_STEPS, activation='softmax')(a)
    a_probs = Permute((2, 1), name='attention_vec')(a)
    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')
    return output_attention_mul

def _create_octconv_residual_block(inputs, ch, N, alpha):
    high, low = inputs
    # OctConv with skip connections
    for i in range(N):
        # adjust channels
        if i == 0:
            skip_high = layers.Conv2D(int(ch*(1-alpha)), 1)(high)
            skip_high = layers.BatchNormalization()(skip_high)
            skip_high = layers.Activation("relu")(skip_high)

            skip_low = layers.Conv2D(int(ch*alpha), 1)(low)
            skip_low = layers.BatchNormalization()(skip_low)
            skip_low = layers.Activation("relu")(skip_low)
        else:
            skip_high, skip_low = high, low

        high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])
        high = layers.BatchNormalization()(high)
        high = layers.Activation("relu")(high)
        low = layers.BatchNormalization()(low)
        low = layers.Activation("relu")(low)

        high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])
        high = layers.BatchNormalization()(high)
        high = layers.Activation("relu")(high)
        low = layers.BatchNormalization()(low)
        low = layers.Activation("relu")(low)

        high = layers.Add()([high, skip_high])
        low = layers.Add()([low, skip_low])
    return [high, low]

def _create_octconv_last_residual_block(inputs, ch, alpha):
    # Last layer for octconv resnets
    high, low = inputs

    # OctConv
    high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])
    high = layers.BatchNormalization()(high)
    high = layers.Activation("relu")(high)
    low = layers.BatchNormalization()(low)
    low = layers.Activation("relu")(low)

    # Last conv layers = alpha_out = 0 : vanila Conv2D
    # high -> high
    high_to_high = layers.Conv2D(ch, 3, padding="same")(high)
    # low -> high
    low_to_high = layers.Conv2D(ch, 3, padding="same")(low)
    low_to_high = layers.Lambda(lambda x:
                        K.repeat_elements(K.repeat_elements(x, 2, axis=1), 2, axis=2))(low_to_high)
    x = layers.Add()([high_to_high, low_to_high])
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x




# build attention LSTM  ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
inputs = Input(shape=(TIME_STEPS, INPUT_DIM))
drop1 = Dropout(0.3)(inputs)
# lstm_out = Bidirectional(CuDNNLSTM(lstm_units, return_sequences=True), name='bilstm')(drop1)
lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm')(drop1)
attention_mul = attention_3d_block(lstm_out)
attention_flatten = Flatten()(attention_mul)
drop2 = Dropout(0.3)(attention_flatten)
lstm_dense_output = Dense(625, activation='sigmoid')(drop2)
lstm_dense_output =  Reshape((25, 25, 1))(lstm_dense_output)
# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑




# build OctConv model linked attention LSTM outputs ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
"""
   Create OctConv Wide ResNet(N=4, k=10)
   """
# Input
input = lstm_dense_output
# downsampling for lower
low = layers.AveragePooling2D(2)(input)

# 16 channels block
high, low = OctConv2D(filters=16, alpha=alpha)([input, low])
high = layers.BatchNormalization()(high)
high = layers.Activation("relu")(high)
low = layers.BatchNormalization()(low)
low = layers.Activation("relu")(low)

# 1st block
high, low = _create_octconv_residual_block([high, low], 16 * k, N, alpha)
# 2nd block
high = layers.AveragePooling2D(2)(high)
low = layers.AveragePooling2D(2)(low)
high, low = _create_octconv_residual_block([high, low], 32 * k, N, alpha)
# 3rd block
high = layers.AveragePooling2D(2)(high)
low = layers.AveragePooling2D(2)(low)
high, low = _create_octconv_residual_block([high, low], 64 * k, N - 1, alpha)
# 3rd block Last
x = _create_octconv_last_residual_block([high, low], 64 * k, alpha)
# FC
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(num_classes, activation="softmax")(x)

model = Model(input, x)

# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
print(model.summary())

print('Training------------')
train_history = model.fit(X_train, y_train,validation_split=0.2, epochs=epochs, batch_size=batch_size)

print('Testing--------------')
loss, accuracy = model.evaluate(X_test, y_test)

print('test loss:', loss)
print('test accuracy:', accuracy)

model.save("F:/数据集/Kim2016/malware_dataset/malware_dataset/mnist_attention_OctConv_model_final.h5")


print("-----------------------DY Add------------------------")
import matplotlib.pyplot as plt


def show_train_history(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title('Train History')
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()


show_train_history(train_history, 'acc', 'val_acc')

show_train_history(train_history, 'loss', 'val_loss')

scores = model.evaluate(X_test, y_test)
print()
print("\t[Info] Accuracy of testing data = {:2.1f}%".format(scores[1] * 100.0))


